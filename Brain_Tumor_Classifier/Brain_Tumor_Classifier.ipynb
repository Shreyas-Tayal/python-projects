{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1yk8Id9AEaQorUS0Y3fFbNm46bquc7T8k",
      "authorship_tag": "ABX9TyMSu3QNWv5sd0kr2gzyfuG1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreyas-Tayal/python-projects/blob/main/Brain_Tumor_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0izzOVjwwHV",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "389a09eb-d69b-4bc2-f2ff-334804c4cc6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras) (3.15.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras) (0.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: keras_cv in /usr/local/lib/python3.12/dist-packages (0.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras_cv) (25.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras_cv) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from keras_cv) (2024.11.6)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.12/dist-packages (from keras_cv) (4.9.9)\n",
            "Requirement already satisfied: keras-core in /usr/local/lib/python3.12/dist-packages (from keras_cv) (0.1.7)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (from keras_cv) (0.3.13)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub->keras_cv) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub->keras_cv) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub->keras_cv) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras-core->keras_cv) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras-core->keras_cv) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras-core->keras_cv) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras-core->keras_cv) (3.15.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.12/dist-packages (from keras-core->keras_cv) (0.1.9)\n",
            "Requirement already satisfied: array_record>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras_cv) (0.8.2)\n",
            "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (1.13.0)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras_cv) (4.2.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras_cv) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras_cv) (5.29.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras_cv) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras_cv) (18.1.0)\n",
            "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras_cv) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras_cv) (1.17.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras_cv) (3.2.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras_cv) (0.10.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras_cv) (2.0.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (0.8.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (2025.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (6.5.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (4.15.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->keras_cv) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->keras_cv) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->keras_cv) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->keras_cv) (2025.10.5)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.12/dist-packages (from dm-tree->keras-core->keras_cv) (25.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from promise->tensorflow-datasets->keras_cv) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras-core->keras_cv) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras-core->keras_cv) (2.19.2)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.12/dist-packages (from simple_parsing->tensorflow-datasets->keras_cv) (0.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.12/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras_cv) (1.71.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras_cv) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras\n",
        "!pip install keras_cv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Setting up the imports\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "import keras_cv\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "1AIMT2t6Pk0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up datasets\n",
        "img_size = [256,256]\n",
        "\n",
        "# Correcting the filepath\n",
        "filepath_data = '/content/drive/MyDrive/Brain_Tumor_Data/Data' # Assuming 'Training' is the correct directory\n",
        "filepath_ver = '/content/drive/MyDrive/Brain_Tumor_Data/Validation'\n",
        "\n",
        "#tf.data.Dataset is a representation of the dataset (Generator)\n",
        "main_dataset = keras.utils.image_dataset_from_directory(\n",
        "    filepath_data,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    batch_size=32,\n",
        "    image_size=img_size\n",
        ")\n",
        "\n",
        "#print(len(main_dataset))\n",
        "\n",
        "train_size = int(len(main_dataset)*0.8)\n",
        "test_size = int(len(main_dataset)*0.2)\n",
        "\n",
        "train_dataset = main_dataset.take(train_size)\n",
        "test_dataset = main_dataset.skip(train_size).take(test_size)\n",
        "\n",
        "#print(len(train_dataset))\n",
        "\n",
        "# Turns the tf.data.Dataset into a numpy array\n",
        "train_iterator = train_dataset.as_numpy_iterator()\n",
        "test_iterator = test_dataset.as_numpy_iterator()\n",
        "\n",
        "# Takes a batch from the array\n",
        "train = train_iterator.next()\n",
        "test = test_iterator.next()\n",
        "\n",
        "#print(train[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "dB0iRNUxS8ZC",
        "outputId": "8f7e98c2-2eed-4fea-e480-0a87c7da4614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "Could not find directory /content/drive/MyDrive/Brain_Tumor_Data/Data",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4147096968.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#tf.data.Dataset is a representation of the dataset (Generator)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m main_dataset = keras.utils.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mfilepath_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inferred'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/image_dataset_utils.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     image_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"inferred\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0msubdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    766\u001b[0m   \"\"\"\n\u001b[1;32m    767\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     raise errors.NotFoundError(\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0mnode_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Could not find directory /content/drive/MyDrive/Brain_Tumor_Data/Data"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AlqdcvqHUbrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_img, train_label = train\n",
        "\n",
        "test_img, test_label = test\n",
        "\n",
        "# Normalize training and testing data (We do this as the image input in the eval phase also has the same happen to it)\n",
        "train_img = train_img / 255.0\n",
        "test_img = test_img / 255.0\n",
        "# The abover statements normalize the data in the range [0, 1]\n",
        "\n",
        "print(\"train_labels shape:\", train_label.shape)\n",
        "\n",
        "if len(train_label) == 1:  # If labels are not one-hot encoded (add .shape if it doesnt work)\n",
        "    train_label = keras.utils.to_categorical(train_label, num_classes=4)\n",
        "    test_label = keras.utils.to_categorical(test_label, num_classes=4)\n",
        "\n",
        "print(\"train_labels shape:\", train_label.shape)\n",
        "\n",
        "# Tensorboard and backup model setup\n",
        "logdir = '/content/drive/MyDrive/Brain_Tumor_Data/logs'\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "backup = tf.keras.callbacks.ModelCheckpoint(filepath='/content/drive/MyDrive/Brain_Tumor_Data/logs/backup_model.keras', verbose=2)\n",
        "\n",
        "\n",
        "# Debugging tool\n",
        "tf.debugging.experimental.enable_dump_debug_info(logdir, tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
        "\n",
        "# Input layer (batch_size here is unecessary, keras handles that by default)\n",
        "img_input = keras.Input(shape=(256, 256, 3))\n",
        "print(\"Input shape:\", img_input.shape)\n",
        "\n",
        "# Creates CNN layers\n",
        "conv1 = keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu')(img_input)\n",
        "pooling1 = keras.layers.MaxPool2D()(conv1)\n",
        "\n",
        "conv2 = keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu')(pooling1)\n",
        "pooling2 = keras.layers.MaxPool2D()(conv2)\n",
        "\n",
        "conv3 = keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu')(pooling2)\n",
        "pooling3 = keras.layers.MaxPool2D()(conv3)\n",
        "\n",
        "flatten = keras.layers.Flatten()(pooling3)\n",
        "\n",
        "dense1 = keras.layers.Dense(256, activation='relu')(flatten)\n",
        "\n",
        "output = keras.layers.Dense(4, activation='sigmoid')(dense1)\n",
        "\n",
        "# Creates the model\n",
        "model = keras.Model(inputs=img_input, outputs=output, name=\"test_model\")\n",
        "model.summary()\n",
        "\n",
        "# Compiles the model with an Adam optimizer\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Check that the data is loaded properly (Ensure train[0] and test[0] are valid NumPy arrays)\n",
        "print(\"Training data shape:\", train[0].shape)\n",
        "print(\"Validation data shape:\", test[0].shape)\n",
        "\n",
        "# Fit the model with the training and validation data\n",
        "history = model.fit(\n",
        "    train_img,  # Training data (ensure it's a valid NumPy array)\n",
        "    train_label,  # Training labels (ensure it's a valid NumPy array)\n",
        "    epochs=10,\n",
        "    validation_data=(test_img, test_label),  # Tests data (If this doesn't work, try something like test only, without defining the tuples)\n",
        "    callbacks=[tensorboard_callback, backup],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Making predictions\n",
        "# y_pred = model.predict(test_data)  # Use the appropriate test data here\n",
        "# print(\"Predictions:\", y_pred)\n"
      ],
      "metadata": {
        "id": "EgFvokznz_R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vrIfRuvqucEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jl8rjnF4pptW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = files.upload()"
      ],
      "metadata": {
        "id": "tCpBw5AxP8N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv.imread('/content/drive/MyDrive/Brain_Tumor_Data/Validation/glioma_ver/0001.jpg')\n",
        "#plt.imshow(img)\n",
        "#plt.show()\n",
        "#np.expand_dims(img/255, 0).shape -----> Will give an extra array, or put simply, it will put the entire array into a new array\n",
        "img = cv.resize(img,(256, 256))\n",
        "yhat = model.predict(np.expand_dims(img/255, 0)) #We divide img/255 to make sure the values are between 0 and 1, which is easier to work with\n",
        "yhat = np.squeeze(yhat)\n",
        "\n",
        "\n",
        "print(yhat)\n",
        "n=0\n",
        "mx = yhat[0]\n",
        "\n",
        "for i in range(len(yhat)):\n",
        "  if yhat[i] >= mx:\n",
        "      mx = yhat[i]\n",
        "      n = i\n",
        "\n",
        "print(n)\n",
        "\n",
        "if n == 0:\n",
        "  print(\"The given image is of a glioma tumor.\")\n",
        "elif n == 1:\n",
        "  print(\"The given image is of a healthy brain.\")\n",
        "elif n == 2:\n",
        "  print(\"The given image is of a meningioma tumor.\")\n",
        "elif n == 3:\n",
        "  print(\"The given image is of a pituitary tumor.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "r5WDr1W_a_-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '/content/drive/MyDrive/Brain_Tumor_Data/logs'"
      ],
      "metadata": {
        "id": "nw_lBt4ja-Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_for_none(data):\n",
        "  \"\"\"Checks for None values in the dataset.\"\"\"\n",
        "  for i in range(len(data)):\n",
        "    if data[i] is None:\n",
        "      print(f\"Found None value at index {i}\")\n",
        "      return True  # Return True if None is found\n",
        "  return False  # Return False if no None is found\n",
        "\n",
        "if check_for_none(train[0]) or check_for_none(train[1]) or check_for_none(test[0]) or check_for_none(test[1]):\n",
        "  print(\"Dataset contains None values. Please check your data loading and preprocessing.\")\n",
        "else:\n",
        "  print(train[0].shape)\n",
        "  print(np.any(np.isnan(train[0])))\n",
        "\n",
        "  # Check a few samples of your data from the generator\n",
        "for data_batch, labels_batch in data_generator:\n",
        "    print(\"Data batch shape:\", data_batch.shape)\n",
        "    print(\"Labels batch shape:\", labels_batch.shape)\n",
        "    break  # Exit after checking the first batch\n"
      ],
      "metadata": {
        "id": "S5iv1NNMpn49"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}